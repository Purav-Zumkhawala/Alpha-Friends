{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MZMbGCw0Qxfg"
   },
   "source": [
    "# **Finetuning Vanilla GPT2 using HuggingFace and Tensorflow**\n",
    "\n",
    "This notebook uses the hugginface finefuning scripts and then uses the TensorFlow version of the genreated models. To generate the texts for a traget character."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PzB7m0GI5fSt"
   },
   "source": [
    "First begin setup by cloning transformers repo. We need to store the training script locally as that would be the most easy way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "91rmSAUQVIUP",
    "outputId": "d1de2196-0eed-48eb-9ef2-d9c2129ca85e"
   },
   "outputs": [],
   "source": [
    "#Clone the transformers repo into the notebook\n",
    "!git clone https://github.com/huggingface/transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XOTdb4rWv8YN"
   },
   "source": [
    "Change directory location to be in the examples folder and then install any requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K2M2Oz9CYB4P",
    "outputId": "997fd59c-c5df-4bd1-b1d3-b2489c4848bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C is OS\n",
      " Volume Serial Number is CAB1-400D\n",
      "\n",
      " Directory of C:\\Users\\purav\\Downloads\\Study\\NLP\\Project\\Project github\\Alpha-Friends\\Model\\vanilla-gpt2\\transformers\\examples\\pytorch\\language-modeling\n",
      "\n",
      "04/22/2021  12:04 AM    <DIR>          .\n",
      "04/22/2021  12:04 AM    <DIR>          ..\n",
      "04/22/2021  12:04 AM           350,616 eval_tmp.txt\n",
      "04/21/2021  11:47 PM             6,621 README.md\n",
      "04/21/2021  11:47 PM                54 requirements.txt\n",
      "04/21/2021  11:47 PM            19,545 run_clm.py\n",
      "04/21/2021  11:47 PM            18,487 run_clm_no_trainer.py\n",
      "04/21/2021  11:47 PM            21,035 run_mlm.py\n",
      "04/21/2021  11:47 PM            20,609 run_mlm_no_trainer.py\n",
      "04/21/2021  11:47 PM            19,617 run_plm.py\n",
      "04/22/2021  12:04 AM         3,049,519 train_tmp.txt\n",
      "               9 File(s)      3,506,103 bytes\n",
      "               2 Dir(s)  72,743,759,872 bytes free\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"transformers\")\n",
    "os.chdir(\"./examples/pytorch/language-modeling\")\n",
    "!Dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "04rBGxwiYnep",
    "outputId": "d4403431-01ac-4987-cebd-e6ea41e28e08"
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eo5gRmXaWx0m",
    "outputId": "d32d984f-73ed-419e-f270-a8e35a6178c4"
   },
   "outputs": [],
   "source": [
    "!pip install pyarrow --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a6G6WINaYmwx",
    "outputId": "f316c525-e595-4ed3-c141-7c41868c117c"
   },
   "outputs": [],
   "source": [
    "# Need to install latest transformer packages from github so the scripts will run correctly\n",
    "! pip install git+git://github.com/huggingface/transformers/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oDuJnVmrY_Fb"
   },
   "source": [
    "Load the data created previously and split it in to train and test. Change the file name as per requirement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uU4ckPTf9T-2",
    "outputId": "85a517e0-17ca-4582-fe7c-090868d173e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training size:2796\n",
      "Evaluation size: 311\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Now load the data line by line\n",
    "\"\"\"\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "with open('../../../../../../Data/Ross_dialogues_all.txt', 'r',encoding='utf-8') as data:\n",
    "    dataset = [\"<|title|>\" + x.strip() for x in data.readlines()]\n",
    "\n",
    "train, eval = train_test_split(dataset, train_size=.9, random_state=2020)\n",
    "print(\"training size:\" + str(len(train)))\n",
    "print(\"Evaluation size: \" + str(len(eval)))\n",
    "\n",
    "with open('train_tmp.txt', 'w') as file_handle:\n",
    "    file_handle.write(\"<|endoftext|>\".join(train))\n",
    "\n",
    "with open('eval_tmp.txt', 'w') as file_handle:\n",
    "    file_handle.write(\"<|endoftext|>\".join(eval))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1TYi8Oqs6Npo"
   },
   "source": [
    "The script below will fine tune GPT2 on your text data that we setup above. This training step will take anywhre from tens of minutes to hours depending on how large your training set is, how many epochs you intend to train on, and if you are using Gpu or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yyV_rTL3ZE_H",
    "outputId": "f95ccdf6-39e3-4bd0-f42c-be4ee081a96e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-22 00:07:22.725517: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
      "2021-04-22 00:07:22.726180: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "Traceback (most recent call last):\n",
      "  File \"run_clm.py\", line 457, in <module>\n",
      "    main()\n",
      "  File \"run_clm.py\", line 182, in main\n",
      "    model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n",
      "  File \"C:\\Users\\purav\\anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\hf_argparser.py\", line 187, in parse_args_into_dataclasses\n",
      "    obj = dtype(**inputs)\n",
      "  File \"<string>\", line 67, in __init__\n",
      "  File \"C:\\Users\\purav\\anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\training_args.py\", line 581, in __post_init__\n",
      "    \"Mixed precision training with AMP or APEX (`--fp16`) and FP16 evaluation can only be used on CUDA devices.\"\n",
      "ValueError: Mixed precision training with AMP or APEX (`--fp16`) and FP16 evaluation can only be used on CUDA devices.\n"
     ]
    }
   ],
   "source": [
    "!python run_clm.py \\\n",
    "--model_type gpt2 \\\n",
    "--model_name_or_path gpt2 \\\n",
    "--train_file \"train_tmp.txt\" \\\n",
    "--do_train \\\n",
    "--validation_file \"eval_tmp.txt\" \\\n",
    "--do_eval \\\n",
    "--per_gpu_train_batch_size 1 \\\n",
    "--save_steps -1 \\\n",
    "--num_train_epochs 5 \\\n",
    "--fp16 \\\n",
    "--output_dir=\"../../../..Ross_model\" \\\n",
    "--overwrite_output_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3CpzI5mU1jPl"
   },
   "source": [
    "# **Using the model**\n",
    "Next lets take the model we just trained and use it to generate some text! We will import the Tensorflow version of the gpt2 language model and set the from_pt flag to True. Then we load a pretrained tokenizer from huggingface. This may take some time to download the tokenizer data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 275,
     "referenced_widgets": [
      "ee86d580a35b4d4ca07fe0b923738e52",
      "454c0b79efe847ccb69e96ab0af684d7",
      "8c6fae3b326b411a8a0a2b6c8ffb2c79",
      "33eb3d9e4df1456bada1a9352fdc4dbd",
      "9527d0bb180f450289d44f4535601570",
      "0a977af99042404ebb738c87c4ceeef9",
      "c282008e229641919ed30903ff2b5b60",
      "13ad07d2ec424e89aacaf8a1e11abbce",
      "712c2e83046642c583cca7d52bc0935f",
      "5bc82b61f11648d99be90c1a173aeab0",
      "9ce2c0fed7ee4fd788d0e59d6c05e466",
      "f6f7c1cef7de44f28eae8a46bb6ef617",
      "a2f62c756ff94086b4e224913666aa38",
      "d0afa74155d1429a85a61bbc1a44e66f",
      "04c7dd9abdc8484dae8c58290ceb8082",
      "a25a3a2d85c64c6880df3122250617e0",
      "12e2335dbfcc4095999fc857261ae3e4",
      "7796c8b39c8a48e5bcfeb39c1a9e4c4b",
      "004e416115e648f7ae83f60e40affad0",
      "dbf4f26db1f6476dad58f90d00c17f11",
      "2703db271ec34fa09d8ea31bce5a8151",
      "8e04d9624d6d4cf2b0a9d3bbf5c582c5",
      "483741de703f47e7b0eb72a3a5fc7d66",
      "83b3ea5742bd4d7895cc1b6b2e413035"
     ]
    },
    "id": "kFOx9AUa1tnk",
    "outputId": "16748049-4561-4d60-eceb-241fb6c942d6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFGPT2LMHeadModel: ['transformer.h.5.attn.masked_bias', 'transformer.h.3.attn.masked_bias', 'transformer.h.0.attn.masked_bias', 'transformer.h.6.attn.masked_bias', 'transformer.h.2.attn.masked_bias', 'lm_head.weight', 'transformer.h.7.attn.masked_bias', 'transformer.h.8.attn.masked_bias', 'transformer.h.9.attn.masked_bias', 'transformer.h.4.attn.masked_bias', 'transformer.h.10.attn.masked_bias', 'transformer.h.11.attn.masked_bias', 'transformer.h.1.attn.masked_bias']\n",
      "- This IS expected if you are initializing TFGPT2LMHeadModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFGPT2LMHeadModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e386e79e4e9341dbb836d8adba99c0e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1042301.0, style=ProgressStyle(descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a67da590037047d3a256fe95b23fc76f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=456318.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d70797948794509b32afafa36735a12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1355256.0, style=ProgressStyle(descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# setup imports to use the model\n",
    "from transformers import TFGPT2LMHeadModel\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "model = TFGPT2LMHeadModel.from_pretrained(\"../../../../Ross_model\", from_pt=True)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To generate only samples from one sentence input use the below cell blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3oWWo4HJ4wd-"
   },
   "source": [
    "We fitst encode the input using the pretrained tokenizer. Next we will use the model to generate the text from our input sample. The parameters I used are based on trial and error from playing around with the huggingface tutorial, https://huggingface.co/blog/how-to-generate, which really goes into great detail on how to go about finding the best parameters for generating text. As well they dive into really good information on what each parameter does and how they play into one another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NbzHNvvaAPns",
    "outputId": "5f48aaeb-87b6-4675-855d-b51a4ad8648f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Oh it's OK. You were worth the wait, and I don't just mean tonight.<Ross>Yeah, of course you were too late for me!<endRoss><Ross>: What about this big fight with her?<endsRach: Come on Rachel, get out there! Get back in your chair!Hey Phoebe!Come here.We're not getting married!!I'll see you at work tomorrow afternoon.Okay, when do we start working on that wedding ring?!When are they supposed to be ready by today anyway??Phyllis got a little busy Monday morning...Oh my God! Is she gonna make dinner while we go?Monica, Monica will have lunch later than any other time before!Wait Chandler\n",
      "\n",
      "1: Oh it's OK. You were worth the wait, and I don't just mean tonight.<Ross>What?<endMonica>: A little later than we'd have liked, but this is a really cool date.Hey!So you're back from Mexico.Yeah! So what are ya gonna do tomorrow night with my grandmother?!Oh yeah-no. Y'know um...<Richard starts kissing Monica on her pillow>Uh Rach umm, okay if yay, maybe uh...We could actually get that couch here in-in Mexico for uh uh, where they'd pay us something.<Melissa kisses Richard across his back while he makes eye contact><Rach: Look Joey that was probably a good idea too.<K\n",
      "\n",
      "2: Oh it's OK. You were worth the wait, and I don't just mean tonight.<Ross>Well, y'know what? We're gonna go check on her again this afternoon.<endRoss><Ross>: Okay, well uh...<endRach: Now if you can tell me where she is in a couple minutes then maybe we should give her some time to relax.You know why not?She obviously wants nothing more than another three hours with that stupid girl. She may have an axe for two years!No Ross. Don-Don-don-you like her, she has always wanted a boyfriend but none of my friends seem so comfortable around any guy other close by now who's very strong enough emotionally or physically capable yet\n",
      "\n",
      "3: Oh it's OK. You were worth the wait, and I don't just mean tonight.<Ross>Well then...<endRoss><Ross>: \"I'll see you at 5:30.\" Wow, this is so nice! So we can have that last one before he leaves!!<endsRach?\n",
      "\n",
      "4: Oh it's OK. You were worth the wait, and I don't just mean tonight.<Ross>What?<endMonica><Ross>\"Tonight was a nightmare.\"\"Nightmare.\"<endsDana>: \"No. No one ever told me that.\"The other night at work they took all my information away from us so we could make a deal with a drug dealer!We had no idea what the deal would be.All right look uh, y'know how Chandler's best friend will be getting married tomorrow evening.I guess he doesn...What are you talking about?! That guy is gonna get divorced by then!That never happened to Monica or Ross!Well hey buddy let 'em trade names because when they do that people hate\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Change the character name here <Ross> to <Character> for the character you trained your model on\n",
    "input_ids = tokenizer.encode(\"Oh it's OK. You were worth the wait, and I don't just mean tonight.<Ross>\", return_tensors='tf')\n",
    "generated_text_samples = model.generate(\n",
    "    input_ids, \n",
    "    max_length=150,  \n",
    "    num_return_sequences=5,\n",
    "    no_repeat_ngram_size=2,\n",
    "    repetition_penalty=1.5,\n",
    "    top_p=0.92,\n",
    "    temperature=.85,\n",
    "    do_sample=True,\n",
    "    top_k=125,\n",
    "    early_stopping=True\n",
    ")\n",
    "#Print output for each sequence generated above\n",
    "for i, beam in enumerate(generated_text_samples):\n",
    "    print(\"{}: {}\".format(i,tokenizer.decode(beam, skip_special_tokens=True)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To generate sentences and interact with the model run the following cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First the system would ask you to start a conversation. After that based on the value of \"num_return_sequences\" the model system would generate those many reposnses. You will need to select the response you liked the most by entering values from 0-5 as for now but this can be changed. After that you can add your response to the generated text and the model would then take the whole conversation that has happened till now and generate further responses using the previous converation as history in form of input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 763
    },
    "id": "K3lgbYoTOxtj",
    "outputId": "8c89a022-c1d9-4264-9bf1-48237e1a21a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: HI  |: Hi!\n",
      "\n",
      "1: HI  |: Ah, hi. Oh my God. Hi!\n",
      "\n",
      "2: HI  |: Hi!\n",
      "\n",
      "3: HI  |: Hi.\n",
      "\n",
      "4: HI  |: Hello, Joey?\n",
      "\n",
      "Enter exit to exit system or the number to the resposne you like4\n",
      "YOU:No it is chandler\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HI Hello, Joey?No it is chandler\n",
      "0: HI Hello, Joey?No it is chandler  |: Hey Chandler. I'm not gonna miss tonight.\n",
      "\n",
      "1: HI Hello, Joey?No it is chandler  |: Hey.\n",
      "\n",
      "2: HI Hello, Joey?No it is chandler  |: What?\n",
      "\n",
      "3: HI Hello, Joey?No it is chandler  |: Hi!\n",
      "\n",
      "4: HI Hello, Joey?No it is chandler  |: Hi!\n",
      "\n",
      "Enter exit to exit system or the number to the resposne you like0\n",
      "YOU:You better not\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HI Hello, Joey?No it is chandler Hey Chandler. I'm not gonna miss tonight.You better not\n",
      "0: HI Hello, Joey?No it is chandler Hey Chandler. I'm not gonna miss tonight.You better not  |: That's right! No-no you don't mind if we get to do that tomorrow night!\n",
      "\n",
      "1: HI Hello, Joey?No it is chandler Hey Chandler. I'm not gonna miss tonight.You better not  |: All right okay, what's wrong with you guys anyway?!\n",
      "\n",
      "2: HI Hello, Joey?No it is chandler Hey Chandler. I'm not gonna miss tonight.You better not  |: But you can't stay a week without saying that.\n",
      "\n",
      "3: HI Hello, Joey?No it is chandler Hey Chandler. I'm not gonna miss tonight.You better not  |: I didn't want to ruin this for you.\n",
      "\n",
      "4: HI Hello, Joey?No it is chandler Hey Chandler. I'm not gonna miss tonight.You better not  |: I can't believe this!\n",
      "\n",
      "Enter exit to exit system or the number to the resposne you likeexit\n"
     ]
    }
   ],
   "source": [
    "flag = True\n",
    "count =0\n",
    "# change the start and end str accddingly for different characters\n",
    "start_str = \"<Ross>\"\n",
    "end_str = \"<endRoss>\"\n",
    "while flag:\n",
    "    if count==0:\n",
    "        X = input(\"Start the conversation:  \")\n",
    "        count+=1\n",
    "    if X ==\"exit\":\n",
    "        flag=False\n",
    "        continue\n",
    "\n",
    "    input_ids = tokenizer.encode(X+\" \"+start_str, return_tensors='tf')\n",
    "    generated_text_samples = model.generate(\n",
    "        input_ids, \n",
    "        max_length=150,\n",
    "        num_return_sequences=5,\n",
    "        no_repeat_ngram_size=2,\n",
    "        repetition_penalty=1.5,\n",
    "        top_p=0.92,\n",
    "        temperature=.85,\n",
    "        do_sample=True,\n",
    "        top_k=125,\n",
    "        early_stopping=True\n",
    "      )\n",
    "      #Print output for each sequence generated above\n",
    "    for i, beam in enumerate(generated_text_samples):\n",
    "        temp = tokenizer.decode(beam, skip_special_tokens=True)\n",
    "        start_index = temp.find(end_str)\n",
    "        end_index = start_index + len(end_str)\n",
    "        result = temp[:start_index]\n",
    "        s_i = result.find(start_str)\n",
    "        result = result.replace(start_str,\"\")\n",
    "        print(\"{}: {} |: {}\".format(i,result[:s_i],result[s_i:]))\n",
    "        print()\n",
    "        \n",
    "    choice = -1\n",
    "    choice = input(\"Enter exit to exit system or the number to the resposne you like  \")\n",
    "    if choice==\"exit\":\n",
    "        flag = False\n",
    "        continue\n",
    "    if choice!=\"exit\" and choice in [\"0\",\"1\",\"2\",\"3\",\"4\"]:\n",
    "        for i, beam in enumerate(generated_text_samples):\n",
    "            if i == (int)(choice):\n",
    "                temp = tokenizer.decode(beam, skip_special_tokens=True)\n",
    "                start_index = temp.find(end_str)\n",
    "                end_index = start_index + len(end_str)\n",
    "                result = temp[:start_index]\n",
    "                result = result.replace(start_str,\"\")\n",
    "                X = result\n",
    "                \n",
    "                \n",
    "    input2 = input(\"YOU: \")\n",
    "    X += input2+\" \"\n",
    "    print(X)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Copy of LM Huggingface finetune GPT-2.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "004e416115e648f7ae83f60e40affad0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8e04d9624d6d4cf2b0a9d3bbf5c582c5",
      "max": 1355256,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2703db271ec34fa09d8ea31bce5a8151",
      "value": 1355256
     }
    },
    "04c7dd9abdc8484dae8c58290ceb8082": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0a977af99042404ebb738c87c4ceeef9": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "12e2335dbfcc4095999fc857261ae3e4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_004e416115e648f7ae83f60e40affad0",
       "IPY_MODEL_dbf4f26db1f6476dad58f90d00c17f11"
      ],
      "layout": "IPY_MODEL_7796c8b39c8a48e5bcfeb39c1a9e4c4b"
     }
    },
    "13ad07d2ec424e89aacaf8a1e11abbce": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2703db271ec34fa09d8ea31bce5a8151": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "33eb3d9e4df1456bada1a9352fdc4dbd": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_13ad07d2ec424e89aacaf8a1e11abbce",
      "placeholder": "​",
      "style": "IPY_MODEL_c282008e229641919ed30903ff2b5b60",
      "value": " 1.04M/1.04M [00:01&lt;00:00, 807kB/s]"
     }
    },
    "454c0b79efe847ccb69e96ab0af684d7": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "483741de703f47e7b0eb72a3a5fc7d66": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5bc82b61f11648d99be90c1a173aeab0": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "712c2e83046642c583cca7d52bc0935f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9ce2c0fed7ee4fd788d0e59d6c05e466",
       "IPY_MODEL_f6f7c1cef7de44f28eae8a46bb6ef617"
      ],
      "layout": "IPY_MODEL_5bc82b61f11648d99be90c1a173aeab0"
     }
    },
    "7796c8b39c8a48e5bcfeb39c1a9e4c4b": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "83b3ea5742bd4d7895cc1b6b2e413035": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8c6fae3b326b411a8a0a2b6c8ffb2c79": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0a977af99042404ebb738c87c4ceeef9",
      "max": 1042301,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9527d0bb180f450289d44f4535601570",
      "value": 1042301
     }
    },
    "8e04d9624d6d4cf2b0a9d3bbf5c582c5": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9527d0bb180f450289d44f4535601570": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "9ce2c0fed7ee4fd788d0e59d6c05e466": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d0afa74155d1429a85a61bbc1a44e66f",
      "max": 456318,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a2f62c756ff94086b4e224913666aa38",
      "value": 456318
     }
    },
    "a25a3a2d85c64c6880df3122250617e0": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a2f62c756ff94086b4e224913666aa38": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "c282008e229641919ed30903ff2b5b60": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d0afa74155d1429a85a61bbc1a44e66f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dbf4f26db1f6476dad58f90d00c17f11": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_83b3ea5742bd4d7895cc1b6b2e413035",
      "placeholder": "​",
      "style": "IPY_MODEL_483741de703f47e7b0eb72a3a5fc7d66",
      "value": " 1.36M/1.36M [00:00&lt;00:00, 5.76MB/s]"
     }
    },
    "ee86d580a35b4d4ca07fe0b923738e52": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8c6fae3b326b411a8a0a2b6c8ffb2c79",
       "IPY_MODEL_33eb3d9e4df1456bada1a9352fdc4dbd"
      ],
      "layout": "IPY_MODEL_454c0b79efe847ccb69e96ab0af684d7"
     }
    },
    "f6f7c1cef7de44f28eae8a46bb6ef617": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a25a3a2d85c64c6880df3122250617e0",
      "placeholder": "​",
      "style": "IPY_MODEL_04c7dd9abdc8484dae8c58290ceb8082",
      "value": " 456k/456k [00:00&lt;00:00, 1.04MB/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
